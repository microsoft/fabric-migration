[
    "spark.acls.enable",
    "spark.admin.acls",
    "spark.admin.acls.groups",
    "spark.app.attempt.id",
    "spark.app.name",
    "spark.appLiveStatusPlugin.defaultTimeout",
    "spark.appLiveStatusPlugins",
    "spark.appStateStore.asyncTracking.enable",
    "spark.archives",
    "spark.authenticate",
    "spark.authenticate.enableSaslEncryption",
    "spark.authenticate.secret",
    "spark.authenticate.secret.driver.file",
    "spark.authenticate.secret.executor.file",
    "spark.authenticate.secret.file",
    "spark.authenticate.secretBitLength",
    "spark.autoscale.executorResourceInfoTag.enabled",
    "spark.barrier.sync.timeout",
    "spark.block.failures.beforeLocationRefresh",
    "spark.blockManager.port",
    "spark.broadcast.UDFCompressionThreshold",
    "spark.broadcast.blockSize",
    "spark.broadcast.checksum",
    "spark.broadcast.compress",
    "spark.buffer.pageSize",
    "spark.buffer.size",
    "spark.buffer.write.chunkSize",
    "spark.checkpoint.compress",
    "spark.cleaner.periodicGC.interval",
    "spark.cleaner.referenceTracking",
    "spark.cleaner.referenceTracking.blocking",
    "spark.cleaner.referenceTracking.blocking.shuffle",
    "spark.cleaner.referenceTracking.cleanCheckpoints",
    "spark.cores.max",
    "spark.dead.worker.persistence",
    "spark.decommission.enabled",
    "spark.default.parallelism",
    "spark.deploy.defaultCores",
    "spark.deploy.maxExecutorRetries",
    "spark.deploy.recoveryDirectory",
    "spark.deploy.recoveryMode",
    "spark.deploy.recoveryMode.factory",
    "spark.deploy.retainedApplications",
    "spark.deploy.retainedDrivers",
    "spark.deploy.spreadOut",
    "spark.deploy.zookeeper.dir",
    "spark.deploy.zookeeper.url",
    "spark.diskStore.subDirectories",
    "spark.driver.appUIAddress",
    "spark.driver.bindAddress",
    "spark.driver.blockManager.port",
    "spark.driver.cores",
    "spark.driver.defaultJavaOptions",
    "spark.driver.extraClassPath",
    "spark.driver.extraJavaOptions",
    "spark.driver.extraLibraryPath",
    "spark.driver.host",
    "spark.driver.log.allowErasureCoding",
    "spark.driver.log.dfsDir",
    "spark.driver.log.layout",
    "spark.driver.log.persistToDfs.enabled",
    "spark.driver.maxResultSize",
    "spark.driver.memory",
    "spark.driver.memoryOverhead",
    "spark.driver.memoryOverheadFactor",
    "spark.driver.port",
    "spark.driver.resourcesFile",
    "spark.driver.supervise",
    "spark.driver.userClassPathFirst",
    "spark.dynamicAllocation.cachedExecutorIdleTimeout",
    "spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled",
    "spark.dynamicAllocation.enabled",
    "spark.dynamicAllocation.executorAllocationRatio",
    "spark.dynamicAllocation.executorIdleTimeout",
    "spark.dynamicAllocation.initialExecutors",
    "spark.dynamicAllocation.maxExecutors",
    "spark.dynamicAllocation.minExecutors",
    "spark.dynamicAllocation.schedulerBacklogTimeout",
    "spark.dynamicAllocation.shuffleTracking.enabled",
    "spark.dynamicAllocation.shuffleTracking.timeout",
    "spark.dynamicAllocation.sustainedSchedulerBacklogTimeout",
    "spark.dynamicAllocation.testing",
    "spark.eventLog.adaptiveFlushing.enabled",
    "spark.eventLog.adaptiveFlushing.threshold",
    "spark.eventLog.adaptiveFlushing.usageEventPeriod",
    "spark.eventLog.buffer.kb",
    "spark.eventLog.compress",
    "spark.eventLog.compression.codec",
    "spark.eventLog.dir",
    "spark.eventLog.enabled",
    "spark.eventLog.erasureCoding.enabled",
    "spark.eventLog.gcMetrics.oldGenerationGarbageCollectors",
    "spark.eventLog.gcMetrics.youngGenerationGarbageCollectors",
    "spark.eventLog.logBlockUpdates.enabled",
    "spark.eventLog.logStageExecutorMetrics",
    "spark.eventLog.longForm.enabled",
    "spark.eventLog.overwrite",
    "spark.eventLog.rolling.enabled",
    "spark.eventLog.rolling.maxFileSize",
    "spark.eventLog.testing",
    "spark.excludeOnFailure.application.fetchFailure.enabled",
    "spark.excludeOnFailure.application.maxFailedExecutorsPerNode",
    "spark.excludeOnFailure.application.maxFailedTasksPerExecutor",
    "spark.excludeOnFailure.enabled",
    "spark.excludeOnFailure.killExcludedExecutors",
    "spark.excludeOnFailure.killExcludedExecutors.decommission",
    "spark.excludeOnFailure.stage.maxFailedExecutorsPerNode",
    "spark.excludeOnFailure.stage.maxFailedTasksPerExecutor",
    "spark.excludeOnFailure.task.maxTaskAttemptsPerExecutor",
    "spark.excludeOnFailure.task.maxTaskAttemptsPerNode",
    "spark.excludeOnFailure.timeout",
    "spark.executor.allowSparkContext",
    "spark.executor.cores",
    "spark.executor.decommission.forceKillTimeout",
    "spark.executor.decommission.killInterval",
    "spark.executor.decommission.signal",
    "spark.executor.defaultJavaOptions",
    "spark.executor.extraClassPath",
    "spark.executor.extraJavaOptions",
    "spark.executor.extraLibraryPath",
    "spark.executor.heartbeat.dropZeroAccumulatorUpdates",
    "spark.executor.heartbeat.maxFailures",
    "spark.executor.heartbeatInterval",
    "spark.executor.id",
    "spark.executor.instances",
    "spark.executor.killOnFatalError.depth",
    "spark.executor.logs.rolling.enableCompression",
    "spark.executor.logs.rolling.maxRetainedFiles",
    "spark.executor.logs.rolling.maxSize",
    "spark.executor.logs.rolling.strategy",
    "spark.executor.logs.rolling.time.interval",
    "spark.executor.memory",
    "spark.executor.memoryOverhead",
    "spark.executor.memoryOverheadFactor",
    "spark.executor.metrics.fileSystemSchemes",
    "spark.executor.metrics.pollingInterval",
    "spark.executor.processTreeMetrics.enabled",
    "spark.executor.pyspark.memory",
    "spark.executor.uri",
    "spark.executor.userClassPathFirst",
    "spark.executorEnv.[EnvironmentVariableName]",
    "spark.extraListeners",
    "spark.files",
    "spark.files.fetchFailure.unRegisterOutputOnHost",
    "spark.files.fetchTimeout",
    "spark.files.ignoreCorruptFiles",
    "spark.files.ignoreMissingFiles",
    "spark.files.io.connectionTimeout",
    "spark.files.maxPartitionBytes",
    "spark.files.openCostInBytes",
    "spark.files.overwrite",
    "spark.files.useFetchCache",
    "spark.graphx.pregel.checkpointInterval",
    "spark.hadoop.cloneConf",
    "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version",
    "spark.hadoop.validateOutputSpecs",
    "spark.hadoopRDD.ignoreEmptySplits",
    "spark.history.aggrelog.container",
    "spark.history.aggrelog.recovery.enabled",
    "spark.history.aggrelog.storageaccount",
    "spark.history.custom.executor.log.url",
    "spark.history.custom.executor.log.url.applyIncompleteApplication",
    "spark.history.fs.cleaner.enabled",
    "spark.history.fs.cleaner.interval",
    "spark.history.fs.cleaner.maxAge",
    "spark.history.fs.cleaner.maxNum",
    "spark.history.fs.driverlog.cleaner.enabled",
    "spark.history.fs.driverlog.cleaner.interval",
    "spark.history.fs.driverlog.cleaner.maxAge",
    "spark.history.fs.endEventReparseChunkSize",
    "spark.history.fs.eventLog.rolling.compaction.score.threshold",
    "spark.history.fs.eventLog.rolling.maxFilesToRetain",
    "spark.history.fs.inProgressOptimization.enabled",
    "spark.history.fs.logDirectory",
    "spark.history.fs.numReplayThreads",
    "spark.history.fs.safemodeCheck.interval",
    "spark.history.fs.update.interval",
    "spark.history.kerberos.enabled",
    "spark.history.kerberos.keytab",
    "spark.history.kerberos.principal",
    "spark.history.multi.tenant.default.account",
    "spark.history.multi.tenant.default.account.key",
    "spark.history.multi.tenant.default.event.folder",
    "spark.history.multi.tenant.enable.swagger",
    "spark.history.multi.tenant.event.cache.maxDiskUsage",
    "spark.history.multi.tenant.event.cache.path",
    "spark.history.multi.tenant.fs.endEventReparseChunkSize",
    "spark.history.multi.tenant.min.retainedApplications",
    "spark.history.multi.tenant.provider",
    "spark.history.multi.tenant.provider.listing.memory.store",
    "spark.history.multi.tenant.provider.listing.memory.store.max.item",
    "spark.history.multi.tenant.retainedApplications",
    "spark.history.multi.tenant.store.maxDiskUsage",
    "spark.history.multi.tenant.store.minFreeDisk",
    "spark.history.multi.tenant.store.path",
    "spark.history.multi.tenant.trace.id.header",
    "spark.history.multi.tenant.ui.port",
    "spark.history.provider",
    "spark.history.retainedApplications",
    "spark.history.store.hybridStore.diskBackend",
    "spark.history.store.hybridStore.enabled",
    "spark.history.store.hybridStore.maxMemoryUsage",
    "spark.history.store.maxDiskUsage",
    "spark.history.store.path",
    "spark.history.ui.acls.enable",
    "spark.history.ui.admin.acls",
    "spark.history.ui.admin.acls.groups",
    "spark.history.ui.maxApplications",
    "spark.history.ui.port",
    "spark.inputOutput.data.enabled",
    "spark.io.compression.codec",
    "spark.io.compression.lz4.blockSize",
    "spark.io.compression.snappy.blockSize",
    "spark.io.compression.zstd.bufferPool.enabled",
    "spark.io.compression.zstd.bufferSize",
    "spark.io.compression.zstd.level",
    "spark.io.crypto.cipher.transformation",
    "spark.io.encryption.enabled",
    "spark.io.encryption.keySizeBits",
    "spark.io.encryption.keygen.algorithm",
    "spark.io.warning.largeFileThreshold",
    "spark.jars",
    "spark.jars.excludes",
    "spark.jars.ivy",
    "spark.jars.ivy.lockStrategy",
    "spark.jars.ivy.retrieve.cleanup",
    "spark.jars.ivy.retrieve.symlink",
    "spark.jars.ivySettings",
    "spark.jars.packages",
    "spark.jars.repositories",
    "spark.jobGroup.sourceMapping.enabled",
    "spark.kafka.consumer.cache.capacity",
    "spark.kafka.consumer.cache.evictorThreadRunInterval",
    "spark.kafka.consumer.cache.jmx.enable",
    "spark.kafka.consumer.cache.timeout",
    "spark.kafka.consumer.fetchedData.cache.evictorThreadRunInterval",
    "spark.kafka.consumer.fetchedData.cache.timeout",
    "spark.kafka.producer.cache.evictorThreadRunInterval",
    "spark.kafka.producer.cache.timeout",
    "spark.kerberos.access.hadoopFileSystems",
    "spark.kerberos.keytab",
    "spark.kerberos.principal",
    "spark.kerberos.relogin.period",
    "spark.kerberos.renewal.credentials",
    "spark.kryo.classesToRegister",
    "spark.kryo.pool",
    "spark.kryo.referenceTracking",
    "spark.kryo.registrationRequired",
    "spark.kryo.registrator",
    "spark.kryo.unsafe",
    "spark.kryoserializer.buffer",
    "spark.kryoserializer.buffer.max",
    "spark.kubernetes.allocation.batch.delay",
    "spark.kubernetes.allocation.batch.size",
    "spark.kubernetes.allocation.driver.readinessTimeout",
    "spark.kubernetes.allocation.executor.timeout",
    "spark.kubernetes.allocation.maxPendingPods",
    "spark.kubernetes.allocation.pods.allocator",
    "spark.kubernetes.appKillPodDeletionGracePeriod",
    "spark.kubernetes.configMap.maxSize",
    "spark.kubernetes.container.image",
    "spark.kubernetes.container.image.pullPolicy",
    "spark.kubernetes.container.image.pullSecrets",
    "spark.kubernetes.context",
    "spark.kubernetes.decommission.script",
    "spark.kubernetes.driver.connectionTimeout",
    "spark.kubernetes.driver.container.image",
    "spark.kubernetes.driver.limit.cores",
    "spark.kubernetes.driver.master",
    "spark.kubernetes.driver.ownPersistentVolumeClaim",
    "spark.kubernetes.driver.pod.featureSteps",
    "spark.kubernetes.driver.pod.name",
    "spark.kubernetes.driver.podTemplateContainerName",
    "spark.kubernetes.driver.podTemplateFile",
    "spark.kubernetes.driver.request.cores",
    "spark.kubernetes.driver.requestTimeout",
    "spark.kubernetes.driver.resourceNamePrefix",
    "spark.kubernetes.driver.reusePersistentVolumeClaim",
    "spark.kubernetes.driver.scheduler.name",
    "spark.kubernetes.driver.service.deleteOnTermination",
    "spark.kubernetes.dynamicAllocation.deleteGracePeriod",
    "spark.kubernetes.executor.apiPollingInterval",
    "spark.kubernetes.executor.checkAllContainers",
    "spark.kubernetes.executor.container.image",
    "spark.kubernetes.executor.decommissionLabel",
    "spark.kubernetes.executor.decommissionLabelValue",
    "spark.kubernetes.executor.deleteOnTermination",
    "spark.kubernetes.executor.disableConfigMap",
    "spark.kubernetes.executor.enablePollingWithResourceVersion",
    "spark.kubernetes.executor.eventProcessingInterval",
    "spark.kubernetes.executor.limit.cores",
    "spark.kubernetes.executor.lostCheck.maxAttempts",
    "spark.kubernetes.executor.minTasksPerExecutorBeforeRolling",
    "spark.kubernetes.executor.missingPodDetectDelta",
    "spark.kubernetes.executor.pod.featureSteps",
    "spark.kubernetes.executor.podNamePrefix",
    "spark.kubernetes.executor.podTemplateContainerName",
    "spark.kubernetes.executor.podTemplateFile",
    "spark.kubernetes.executor.request.cores",
    "spark.kubernetes.executor.rollInterval",
    "spark.kubernetes.executor.rollPolicy",
    "spark.kubernetes.executor.scheduler.name",
    "spark.kubernetes.file.upload.path",
    "spark.kubernetes.hadoop.configMapName",
    "spark.kubernetes.kerberos.krb5.configMapName",
    "spark.kubernetes.kerberos.krb5.path",
    "spark.kubernetes.kerberos.tokenSecret.itemKey",
    "spark.kubernetes.kerberos.tokenSecret.name",
    "spark.kubernetes.local.dirs.tmpfs",
    "spark.kubernetes.memoryOverheadFactor",
    "spark.kubernetes.namespace",
    "spark.kubernetes.pyspark.pythonVersion",
    "spark.kubernetes.report.interval",
    "spark.kubernetes.resource.type",
    "spark.kubernetes.scheduler.name",
    "spark.kubernetes.submission.connectionTimeout",
    "spark.kubernetes.submission.requestTimeout",
    "spark.kubernetes.submission.waitAppCompletion",
    "spark.kubernetes.submitInDriver",
    "spark.kubernetes.trust.certificates",
    "spark.local.dir",
    "spark.locality.wait",
    "spark.locality.wait.legacyResetOnTaskLaunch",
    "spark.locality.wait.node",
    "spark.locality.wait.process",
    "spark.locality.wait.rack",
    "spark.log.callerContext",
    "spark.logConf",
    "spark.master",
    "spark.master.rest.enabled",
    "spark.master.rest.port",
    "spark.master.ui.decommission.allow.mode",
    "spark.master.ui.port",
    "spark.memory.fraction",
    "spark.memory.offHeap.enabled",
    "spark.memory.offHeap.size",
    "spark.memory.storageFraction",
    "spark.mesos.appJar.local.resolution.mode",
    "spark.mesos.cluster.retry.wait.max",
    "spark.mesos.coarse",
    "spark.mesos.coarse.shutdownTimeout",
    "spark.mesos.constraints",
    "spark.mesos.containerizer",
    "spark.mesos.dispatcher.historyServer.url",
    "spark.mesos.dispatcher.queue",
    "spark.mesos.dispatcher.webui.url",
    "spark.mesos.driver.constraints",
    "spark.mesos.driver.failoverTimeout",
    "spark.mesos.driver.frameworkId",
    "spark.mesos.driver.labels",
    "spark.mesos.driver.memoryOverhead",
    "spark.mesos.driver.webui.url",
    "spark.mesos.executor.docker.forcePullImage",
    "spark.mesos.executor.docker.image",
    "spark.mesos.executor.docker.parameters",
    "spark.mesos.executor.docker.portmaps",
    "spark.mesos.executor.docker.volumes",
    "spark.mesos.executor.home",
    "spark.mesos.executor.memoryOverhead",
    "spark.mesos.extra.cores",
    "spark.mesos.fetcherCache.enable",
    "spark.mesos.gpus.max",
    "spark.mesos.maxDrivers",
    "spark.mesos.mesosExecutor.cores",
    "spark.mesos.network.labels",
    "spark.mesos.network.name",
    "spark.mesos.principal",
    "spark.mesos.principal.file",
    "spark.mesos.proxy.baseURL",
    "spark.mesos.rejectOfferDuration",
    "spark.mesos.rejectOfferDurationForReachedMaxCores",
    "spark.mesos.rejectOfferDurationForUnmetConstraints",
    "spark.mesos.retainedDrivers",
    "spark.mesos.role",
    "spark.mesos.secret",
    "spark.mesos.secret.file",
    "spark.mesos.task.labels",
    "spark.mesos.uris",
    "spark.metrics.appStatusSource.enabled",
    "spark.metrics.conf",
    "spark.metrics.executorMetricsSource.enabled",
    "spark.metrics.namespace",
    "spark.metrics.staticSources.enabled",
    "spark.modify.acls",
    "spark.modify.acls.groups",
    "spark.network.crypto.enabled",
    "spark.network.crypto.saslFallback",
    "spark.network.io.preferDirectBufs",
    "spark.network.maxRemoteBlockSizeFetchToMem",
    "spark.network.remoteReadNioBufferConversion",
    "spark.network.timeout",
    "spark.network.timeoutInterval",
    "spark.nonjvm.error.buffer.size",
    "spark.nonjvm.error.forwarding.enabled",
    "spark.plugins",
    "spark.port.maxRetries",
    "spark.pyspark.driver.python",
    "spark.pyspark.python",
    "spark.python.authenticate.socketTimeout",
    "spark.python.daemon.module",
    "spark.python.profile",
    "spark.python.profile.dump",
    "spark.python.task.killTimeout",
    "spark.python.use.daemon",
    "spark.python.worker.faulthandler.enabled",
    "spark.python.worker.memory",
    "spark.python.worker.module",
    "spark.python.worker.reuse",
    "spark.pythonRunnerOutputStream.plugin",
    "spark.r.backendConnectionTimeout",
    "spark.r.command",
    "spark.r.driver.command",
    "spark.r.heartBeatInterval",
    "spark.r.numRBackendThreads",
    "spark.r.shell.command",
    "spark.rdd.checkpoint.cachePreferredLocsExpireTime",
    "spark.rdd.compress",
    "spark.rdd.limit.scaleUpFactor",
    "spark.rdd.parallelListingThreshold",
    "spark.redaction.regex",
    "spark.redaction.string.regex",
    "spark.reducer.maxBlocksInFlightPerAddress",
    "spark.reducer.maxReqsInFlight",
    "spark.reducer.maxSizeInFlight",
    "spark.reset.appName.enabled",
    "spark.resources.discoveryPlugin",
    "spark.resources.warnings.testing",
    "spark.rpc.askTimeout",
    "spark.rpc.connect.threads",
    "spark.rpc.io.backLog",
    "spark.rpc.io.connectionTimeout",
    "spark.rpc.io.numConnectionsPerPeer",
    "spark.rpc.io.threads",
    "spark.rpc.lookupTimeout",
    "spark.rpc.message.maxSize",
    "spark.rpc.netty.dispatcher.numThreads",
    "spark.rpc.numRetries",
    "spark.rpc.retry.wait",
    "spark.scheduler.allocation.file",
    "spark.scheduler.barrier.maxConcurrentTasksCheck.interval",
    "spark.scheduler.barrier.maxConcurrentTasksCheck.maxFailures",
    "spark.scheduler.excludeOnFailure.unschedulableTaskSetTimeout",
    "spark.scheduler.executorTaskExcludeOnFailureTime",
    "spark.scheduler.listenerbus.eventqueue.appStatus.capacity",
    "spark.scheduler.listenerbus.eventqueue.capacity",
    "spark.scheduler.listenerbus.eventqueue.eventLog.capacity",
    "spark.scheduler.listenerbus.eventqueue.executorManagement.capacity",
    "spark.scheduler.listenerbus.eventqueue.shared.capacity",
    "spark.scheduler.listenerbus.eventqueue.streams.capacity",
    "spark.scheduler.listenerbus.eventqueue.timeout",
    "spark.scheduler.listenerbus.logSlowEvent",
    "spark.scheduler.listenerbus.logSlowEvent.threshold",
    "spark.scheduler.listenerbus.metrics.maxListenerClassesTimed",
    "spark.scheduler.maxRegisteredResourcesWaitingTime",
    "spark.scheduler.minRegisteredResourcesRatio",
    "spark.scheduler.mode",
    "spark.scheduler.resource.profileMergeConflicts",
    "spark.scheduler.revive.interval",
    "spark.security.credentials.renewalRatio",
    "spark.security.credentials.retryWait",
    "spark.serializer",
    "spark.serializer.extraDebugInfo",
    "spark.serializer.objectStreamReset",
    "spark.shuffle.accurateBlockSkewedFactor",
    "spark.shuffle.accurateBlockThreshold",
    "spark.shuffle.checksum.algorithm",
    "spark.shuffle.checksum.enabled",
    "spark.shuffle.cleaner.interval",
    "spark.shuffle.compress",
    "spark.shuffle.detectCorrupt",
    "spark.shuffle.detectCorrupt.useExtraMemory",
    "spark.shuffle.file.buffer",
    "spark.shuffle.io.backLog",
    "spark.shuffle.io.connectionTimeout",
    "spark.shuffle.io.maxRetries",
    "spark.shuffle.io.numConnectionsPerPeer",
    "spark.shuffle.io.preferDirectBufs",
    "spark.shuffle.io.retryWait",
    "spark.shuffle.manager",
    "spark.shuffle.mapOutput.dispatcher.numThreads",
    "spark.shuffle.mapOutput.minSizeForBroadcast",
    "spark.shuffle.mapOutput.parallelAggregationThreshold",
    "spark.shuffle.mapStatus.compression.codec",
    "spark.shuffle.maxAccurateSkewedBlockNumber",
    "spark.shuffle.maxAttemptsOnNettyOOM",
    "spark.shuffle.maxChunksBeingTransferred",
    "spark.shuffle.minNumPartitionsToHighlyCompress",
    "spark.shuffle.push.enabled",
    "spark.shuffle.push.finalize.timeout",
    "spark.shuffle.push.maxBlockBatchSize",
    "spark.shuffle.push.maxBlockSizeToPush",
    "spark.shuffle.push.maxRetainedMergerLocations",
    "spark.shuffle.push.merge.finalizeThreads",
    "spark.shuffle.push.mergersMinStaticThreshold",
    "spark.shuffle.push.mergersMinThresholdRatio",
    "spark.shuffle.push.minCompletedPushRatio",
    "spark.shuffle.push.minShuffleSizeToWait",
    "spark.shuffle.push.numPushThreads",
    "spark.shuffle.push.results.timeout",
    "spark.shuffle.push.server.mergedIndexCacheSize",
    "spark.shuffle.push.server.mergedShuffleFileManagerImpl",
    "spark.shuffle.push.server.minChunkSizeInMergedShuffleFile",
    "spark.shuffle.readHostLocalDisk",
    "spark.shuffle.reduceLocality.enabled",
    "spark.shuffle.registration.maxAttempts",
    "spark.shuffle.registration.timeout",
    "spark.shuffle.service.client.class",
    "spark.shuffle.service.db.enabled",
    "spark.shuffle.service.enabled",
    "spark.shuffle.service.fetch.rdd.enabled",
    "spark.shuffle.service.index.cache.size",
    "spark.shuffle.service.name",
    "spark.shuffle.service.port",
    "spark.shuffle.service.removeShuffle",
    "spark.shuffle.sort.bypassMergeThreshold",
    "spark.shuffle.sort.initialBufferSize",
    "spark.shuffle.sort.io.plugin.class",
    "spark.shuffle.sort.useRadixSort",
    "spark.shuffle.spill.batchSize",
    "spark.shuffle.spill.compress",
    "spark.shuffle.spill.diskWriteBufferSize",
    "spark.shuffle.spill.initialMemoryThreshold",
    "spark.shuffle.spill.numElementsForceSpillThreshold",
    "spark.shuffle.sync",
    "spark.shuffle.unsafe.fastMergeEnabled",
    "spark.shuffle.unsafe.file.output.buffer",
    "spark.shuffle.useOldFetchProtocol",
    "spark.sparkContextAfterInit.plugins",
    "spark.sparkr.r.command",
    "spark.speculation",
    "spark.speculation.interval",
    "spark.speculation.minTaskRuntime",
    "spark.speculation.multiplier",
    "spark.speculation.quantile",
    "spark.speculation.task.duration.threshold",
    "spark.stage.maxConsecutiveAttempts",
    "spark.standalone.submit.waitAppCompletion",
    "spark.storage.blockManagerHeartbeatTimeoutMs",
    "spark.storage.blockManagerMasterDriverHeartbeatTimeoutMs",
    "spark.storage.blockManagerTimeoutIntervalMs",
    "spark.storage.cachedPeersTtl",
    "spark.storage.cleanupFilesAfterExecutorExit",
    "spark.storage.decommission.enabled",
    "spark.storage.decommission.fallbackStorage.cleanUp",
    "spark.storage.decommission.fallbackStorage.path",
    "spark.storage.decommission.maxReplicationFailuresPerBlock",
    "spark.storage.decommission.notifyExternalShuffleService",
    "spark.storage.decommission.rddBlocks.enabled",
    "spark.storage.decommission.replicationReattemptInterval",
    "spark.storage.decommission.shuffleBlocks.enabled",
    "spark.storage.decommission.shuffleBlocks.maxDiskSize",
    "spark.storage.decommission.shuffleBlocks.maxThreads",
    "spark.storage.exceptionOnPinLeak",
    "spark.storage.localDiskByExecutors.cacheSize",
    "spark.storage.maxReplicationFailures",
    "spark.storage.memoryMapLimitForTests",
    "spark.storage.memoryMapThreshold",
    "spark.storage.replication.policy",
    "spark.storage.replication.proactive",
    "spark.storage.replication.topologyFile",
    "spark.storage.replication.topologyMapper",
    "spark.storage.unrollMemoryCheckPeriod",
    "spark.storage.unrollMemoryGrowthFactor",
    "spark.storage.unrollMemoryThreshold",
    "spark.streaming.backpressure.enabled",
    "spark.streaming.backpressure.initialRate",
    "spark.streaming.backpressure.pid.derived",
    "spark.streaming.backpressure.pid.integral",
    "spark.streaming.backpressure.pid.minRate",
    "spark.streaming.backpressure.pid.proportional",
    "spark.streaming.backpressure.rateEstimator",
    "spark.streaming.blockInterval",
    "spark.streaming.concurrentJobs",
    "spark.streaming.driver.writeAheadLog.allowBatching",
    "spark.streaming.driver.writeAheadLog.batchingTimeout",
    "spark.streaming.driver.writeAheadLog.class",
    "spark.streaming.driver.writeAheadLog.closeFileAfterWrite",
    "spark.streaming.driver.writeAheadLog.maxFailures",
    "spark.streaming.driver.writeAheadLog.rollingIntervalSecs",
    "spark.streaming.dynamicAllocation.enabled",
    "spark.streaming.dynamicAllocation.maxExecutors",
    "spark.streaming.dynamicAllocation.minExecutors",
    "spark.streaming.dynamicAllocation.scalingDownRatio",
    "spark.streaming.dynamicAllocation.scalingInterval",
    "spark.streaming.dynamicAllocation.scalingUpRatio",
    "spark.streaming.dynamicAllocation.testing",
    "spark.streaming.gracefulStopTimeout",
    "spark.streaming.kafka.allowNonConsecutiveOffsets",
    "spark.streaming.kafka.consumer.cache.enabled",
    "spark.streaming.kafka.consumer.cache.initialCapacity",
    "spark.streaming.kafka.consumer.cache.loadFactor",
    "spark.streaming.kafka.consumer.cache.maxCapacity",
    "spark.streaming.kafka.consumer.poll.ms",
    "spark.streaming.kafka.maxRatePerPartition",
    "spark.streaming.kafka.minRatePerPartition",
    "spark.streaming.manualClock.jump",
    "spark.streaming.receiver.maxRate",
    "spark.streaming.receiver.writeAheadLog.class",
    "spark.streaming.receiver.writeAheadLog.closeFileAfterWrite",
    "spark.streaming.receiver.writeAheadLog.enable",
    "spark.streaming.receiver.writeAheadLog.maxFailures",
    "spark.streaming.receiver.writeAheadLog.rollingIntervalSecs",
    "spark.streaming.sessionByKey.deltaChainThreshold",
    "spark.streaming.stopGracefullyOnShutdown",
    "spark.streaming.ui.retainedBatches",
    "spark.streaming.unpersist",
    "spark.submit.deployMode",
    "spark.submit.pyFiles",
    "spark.task.cpus",
    "spark.task.maxDirectResultSize",
    "spark.task.maxFailures",
    "spark.task.reaper.enabled",
    "spark.task.reaper.killTimeout",
    "spark.task.reaper.pollingInterval",
    "spark.task.reaper.threadDump",
    "spark.taskMetrics.trackUpdatedBlockStatuses",
    "spark.test.noStageRetry",
    "spark.testing",
    "spark.testing.dynamicAllocation.schedule.enabled",
    "spark.testing.memory",
    "spark.testing.nCoresPerExecutor",
    "spark.testing.nExecutorsPerHost",
    "spark.testing.nHosts",
    "spark.testing.reservedMemory",
    "spark.testing.resourceProfileManager",
    "spark.testing.skipValidateCores",
    "spark.ui.advise.hub.impl.class",
    "spark.ui.allowFramingFrom",
    "spark.ui.consoleProgress.update.interval",
    "spark.ui.custom.executor.log.url",
    "spark.ui.dagGraph.retainedRootRDDs",
    "spark.ui.data.maxRecords",
    "spark.ui.enabled",
    "spark.ui.enhancement.allowUpdateCapacity",
    "spark.ui.enhancement.dataTabUrl",
    "spark.ui.enhancement.diagnostic.executor.realtime.enabled",
    "spark.ui.enhancement.diagnostic.executor.realtime.interval",
    "spark.ui.enhancement.diagnostic.executorEventsInsertPointsNum",
    "spark.ui.enhancement.diagnostic.executorEventsSampleThreshold",
    "spark.ui.enhancement.diagnosticTabUrl",
    "spark.ui.enhancement.enabled",
    "spark.ui.enhancement.filter.job.limit",
    "spark.ui.enhancement.filter.stage.extraProps.default",
    "spark.ui.enhancement.filter.stage.limit",
    "spark.ui.enhancement.graphTabUrl",
    "spark.ui.enhancement.maxGraphStages",
    "spark.ui.filters",
    "spark.ui.killEnabled",
    "spark.ui.liveUpdate.minFlushPeriod",
    "spark.ui.liveUpdate.period",
    "spark.ui.port",
    "spark.ui.prometheus.enabled",
    "spark.ui.proxyRedirectUri",
    "spark.ui.requestHeaderSize",
    "spark.ui.retainedDeadExecutors",
    "spark.ui.retainedJobs",
    "spark.ui.retainedStages",
    "spark.ui.retainedTasks",
    "spark.ui.reverseProxy",
    "spark.ui.reverseProxyUrl",
    "spark.ui.showConsoleProgress",
    "spark.ui.strictTransportSecurity",
    "spark.ui.threadDumpsEnabled",
    "spark.ui.timeline.executors.maximum",
    "spark.ui.timeline.jobs.maximum",
    "spark.ui.timeline.stages.maximum",
    "spark.ui.timeline.tasks.maximum",
    "spark.ui.view.acls",
    "spark.ui.view.acls.groups",
    "spark.ui.xContentTypeOptions.enabled",
    "spark.ui.xXssProtection",
    "spark.unsafe.exceptionOnMemoryLeak",
    "spark.unsafe.sorter.spill.read.ahead.enabled",
    "spark.unsafe.sorter.spill.reader.buffer.size",
    "spark.user.groups.mapping",
    "spark.worker.cleanup.appDataTtl",
    "spark.worker.cleanup.enabled",
    "spark.worker.cleanup.interval",
    "spark.worker.decommission.signal",
    "spark.worker.driverTerminateTimeout",
    "spark.worker.executorStateSync.maxAttempts",
    "spark.worker.preferConfiguredMasterAddress",
    "spark.worker.resourcesFile",
    "spark.worker.timeout",
    "spark.worker.ui.compressedLogFileLengthCacheSize",
    "spark.worker.ui.port",
    "spark.worker.ui.retainedDrivers",
    "spark.worker.ui.retainedExecutors"
]