{"cells":[{"cell_type":"code","execution_count":null,"id":"6f71db72-a0e4-4953-83b7-60d378cd6696","metadata":{"microsoft":{"language":"scala"}},"outputs":[],"source":["%%spark\n","// Fabric config\n","var WorkspaceId = \"<workspace_id>\"\n","var LakehouseId = \"<lakehouse_id>\"\n","var IntermediateFolderPath = f\"abfss://${WorkspaceId}@onelake.dfs.fabric.microsoft.com/${LakehouseId}/Files/hms_output/syn/\"\n","\n","var ContainerName = \"<container_name>\"\n","var StorageName = \"<storage_name>\"\n","var SynapseWorkspaceName = <synapse_workspace_name>\n","var WarehouseMappings:Map[String, String] = Map(\n","    f\"abfss://${ContainerName}@${StorageName}.dfs.core.windows.net/synapse/workspaces/${SynapseWorkspaceName}/warehouse\"-> f\"abfss://${WorkspaceId}@onelake.dfs.fabric.microsoft.com/${LakehouseId}/Files/warehouse_dir_syn\",\n","    f\"dbfs:/mnt/${StorageName}/databricks/warehouse\"->f\"abfss://${WorkspaceId}@onelake.dfs.fabric.microsoft.com/${LakehouseId}/Files/warehouse_dir_dbx\",\n","    f\"abfss://${ContainerName}@${StorageName}.dfs.core.windows.net/apps/spark/warehouse\"->f\"abfss://${WorkspaceId}@onelake.dfs.fabric.microsoft.com/${LakehouseId}/Files/warehouse_dir_hdi\"\n",")\n","\n","// Metastore config\n","var DatabasePrefix = \"\"\n","var TablePrefix = \"\"\n","var IgnoreIfExists = true"]},{"cell_type":"code","execution_count":null,"id":"347ad4ed-313b-4836-9b8f-1a0125b00376","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"scala"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%%spark\n","import java.net.URI\n","import java.util.Calendar\n","\n","import scala.collection.mutable.{ListBuffer, Map}\n","import org.apache.spark.sql._\n","import org.apache.spark.sql.types.{ObjectType, _}\n","import org.apache.spark.sql.catalyst._\n","import org.apache.spark.sql.catalyst.catalog._\n","import org.json4s._\n","import org.json4s.JsonAST.JString\n","import org.json4s.jackson.Serialization\n","import org.apache.hadoop.conf.Configuration\n","import org.apache.hadoop.fs.{FileSystem, Path}\n","import org.apache.http.client.methods.{CloseableHttpResponse, HttpPost}\n","import org.apache.http.entity.StringEntity\n","import org.apache.http.impl.client.{CloseableHttpClient, HttpClients}\n","import scala.io.Source\n","\n","\n","var locationPrefixMappingList = WarehouseMappings.toList.sortBy(pair => pair._1).reverse\n","\n","val DatabaseType = \"database\"\n","val TableType = \"table\"\n","val PartitionType = \"partition\"\n","\n","object ImportMetadata {\n","\n","  val spark = SparkSession.builder().getOrCreate()\n","\n","  case object URISerializer extends CustomSerializer[URI](format => ( {\n","    case JString(uri) => new URI(uri)\n","  }, {\n","    case uri: URI => JString(uri.toString())\n","  }))\n","\n","  case object SturctTypeSerializer extends CustomSerializer[StructType](format => ( {\n","    case JString(structType)  => DataType.fromJson(structType).asInstanceOf[StructType]\n","  }, {\n","    case structType: StructType => JString(structType.json)\n","  }))\n","\n","\n","  implicit val formats = DefaultFormats + URISerializer + SturctTypeSerializer\n","\n","  case class CatalogPartitions(database: String, table: String, tablePartitons: Seq[CatalogTablePartition])\n","\n","  case class CatalogTables(database: String, tables: Seq[CatalogTable])\n","\n","  case class CatalogStat(entityType: String, count: Int, database: Option[String], table: Option[String])\n","\n","  def ConvertLocation(location: String) : String = {\n","    var locationMapping = locationPrefixMappingList.find(mapping => {location.startsWith(mapping._1)})\n","\n","    if (locationMapping != None) {\n","      return location.replaceFirst(locationMapping.get._1, locationMapping.get._2)\n","    }\n","\n","    return location;\n","  }\n","\n","  def ConvertCatalogDatabase(database: CatalogDatabase) : CatalogDatabase = {\n","    var convertedDatabase  = new CatalogDatabase(\n","      DatabasePrefix + database.name,\n","      database.description,\n","      new URI(ConvertLocation(database.locationUri.toString())),\n","      database.properties)\n","\n","    return convertedDatabase;\n","  }\n","\n","  def ConvertCatalogStorageFormat(format : CatalogStorageFormat) : CatalogStorageFormat = {\n","\n","    var formatlocation: Option[URI] = None\n","    if (format.locationUri != None) {\n","      formatlocation = Some(new URI(ConvertLocation(format.locationUri.get.toString())))\n","    }\n","\n","    var convertedStorageFormat = new CatalogStorageFormat(\n","      formatlocation,\n","      format.inputFormat,\n","      format.outputFormat,\n","      format.serde,\n","      format.compressed,\n","      format.properties\n","    )\n","\n","    return  convertedStorageFormat;\n","  }\n","\n","  def ConvertCatalogTable(table: CatalogTable) : CatalogTable = {\n","\n","    var dbName = Some(DatabasePrefix + table.identifier.database.get);\n","    var tblName = TablePrefix + table.identifier.table;\n","\n","    var convertedTable = new CatalogTable(\n","      new TableIdentifier(tblName, dbName),\n","      org.apache.spark.sql.catalyst.catalog.CatalogTableType(\"EXTERNAL\"),\n","      ConvertCatalogStorageFormat(table.storage),\n","      table.schema,\n","      table.provider,\n","      table.partitionColumnNames,\n","      table.bucketSpec,\n","      table.owner,\n","      table.createTime,\n","      table.lastAccessTime,\n","      table.createVersion,\n","      table.properties,\n","      table.stats,\n","      table.viewText,\n","      table.comment,\n","      table.unsupportedFeatures,\n","      table.tracksPartitionsInCatalog,\n","      table.schemaPreservesCase,\n","      table.ignoredProperties)\n","\n","    return convertedTable;\n","  }\n","\n","  def ConvertCatalogTablePartition(partition : CatalogTablePartition) : CatalogTablePartition = {\n","    var convertedPartition = new CatalogTablePartition(\n","      partition.spec,\n","      ConvertCatalogStorageFormat(partition.storage),\n","      partition.parameters,\n","      partition.createTime,\n","      partition.lastAccessTime,\n","      partition.stats\n","    );\n","\n","    return convertedPartition;\n","  }\n","\n","  val MaxRetryCount = 3;\n","\n","  def RetriableFunc(func: () => Unit, retryCount: Int = 0): Unit = {\n","    try {\n","      func()\n","    } catch {\n","      case e:Exception => {\n","        if (retryCount < MaxRetryCount){\n","          RetriableFunc(func, retryCount + 1)\n","        } else {\n","          throw e\n","        }\n","      }\n","    }\n","  }\n","\n","  def RetriableQueryFunc(func: () => Object, retryCount: Int = 0): Object = {\n","    try {\n","      func()\n","    } catch {\n","      case e:Exception => {\n","        if (retryCount < MaxRetryCount){\n","          RetriableQueryFunc(func, retryCount + 1)\n","        } else {\n","          throw e\n","        }\n","      }\n","    }\n","  }\n","\n","// Create DBs\n","\n","  def CreateDatabases(dataPath: String): Int = {\n","\n","    println(\"Start to create databases \" + Calendar.getInstance().getTime())\n","\n","    val ds = spark.read.format(\"text\").load(dataPath)\n","\n","    var createdCount = 0;\n","    var existsDbs = spark.sharedState.externalCatalog.listDatabases()\n","    var data = ds.collect()\n","    var total = data.size\n","\n","    data.foreach(row => {\n","      var jsonString = row.getString(0)\n","      var newDb = ConvertCatalogDatabase(Serialization.read[CatalogDatabase](jsonString))\n","\n","      var exists = existsDbs.contains(newDb.name)\n","      if (exists && !IgnoreIfExists) {\n","\n","        println(createdCount + \"/\" + total + \" databases created. \" + Calendar.getInstance().getTime())\n","        println(\"Database \" + newDb.name + \" already exists\")\n","\n","        throw new Exception(\"Database \" + newDb.name + \" already exists\")\n","      } else if (!exists) {\n","        CreateDatabase(newDb.name)\n","      }\n","\n","      createdCount+=1;\n","\n","      if (createdCount%100 == 0) {\n","        println(createdCount + \"/\" + total + \" databases created\" + Calendar.getInstance().getTime())\n","      }\n","    });\n","\n","    println(\"Databases Created completed. Total \" + createdCount + \" database created. \" + Calendar.getInstance().getTime())\n","    return createdCount\n","  }\n","\n","  def CreateDatabase(dbName: String) = {\n","    mssparkutils.lakehouse.create(dbName, \"imported db\", WorkspaceId)\n","  }\n","\n","  // Create Tables\n","\n","  def CreateTables(dataPath: String): Int = {\n","    println(\"Start to create tables \" + Calendar.getInstance().getTime())\n","\n","    val ds = spark.read.format(\"text\").load(dataPath);\n","\n","    var createdCount = 0;\n","    ds.collect().foreach(row => {\n","      var jsonString = row.getString(0)\n","      var tables = Serialization.read[CatalogTables](jsonString);\n","\n","      var existsTables = spark.sharedState.externalCatalog.listTables(DatabasePrefix + tables.database)\n","      var perTables = tables.tables.toParArray\n","\n","      perTables.foreach(table => {\n","        var newTable = ConvertCatalogTable(table)\n","        var exists = existsTables.contains(newTable.identifier.table)\n","        if (exists && !IgnoreIfExists) {\n","\n","          println(createdCount + \" tables created. \" + Calendar.getInstance().getTime())\n","          println(\"Table \" + newTable.identifier.database + \".\" + newTable.identifier.table + \" already exists\")\n","\n","          throw new Exception(\"Table \" + newTable.identifier.database + \".\" + newTable.identifier.table + \" already exists\")\n","        } else if (!exists) {\n","          CreateTable(newTable)\n","        }\n","\n","        createdCount+=1;\n","      })\n","\n","      println(createdCount + \" tables created\" + Calendar.getInstance().getTime())\n","    })\n","\n","    println(\"Tables Created completed. Total \" + createdCount + \" table created. \" + Calendar.getInstance().getTime())\n","    return createdCount\n","  }\n","\n","  def CreateTable(table:CatalogTable) = {\n","    RetriableFunc(() => {\n","      spark.sharedState.externalCatalog.createTable(table, IgnoreIfExists)\n","    })\n","  }\n","\n","  def ValidateTablePath(dataPath: String) = {\n","    println(\"Start to validate table path \" + Calendar.getInstance().getTime())\n","\n","    val ds = spark.read.format(\"text\").load(dataPath)\n","    var hadoopConf = spark.sparkContext.hadoopConfiguration\n","\n","    ds.collect().foreach(row => {\n","      var jsonString = row.getString(0)\n","      var tables = Serialization.read[CatalogTables](jsonString);\n","\n","      tables.tables.toParArray.foreach(table => {\n","        var newTable = ConvertCatalogTable(table)\n","        try{\n","          var p = new Path(newTable.location);\n","          var fs = p.getFileSystem(hadoopConf);\n","        } catch {\n","          case e:Exception => {\n","            throw new Exception(\"Validate table path failed. Table: \" + newTable.identifier.database.getOrElse() + \".\" + newTable.identifier.table + \", Location: \" +  newTable.location + \" , exception: \" + e)\n","          }\n","        }\n","      })\n","    })\n","\n","    println(\"Validate table path completed\")\n","  }\n","\n","  // Create Partitions\n","\n","  def CreatePartitions(dataPath: String): Int = {\n","    println(\"Start to create partitions \" + Calendar.getInstance().getTime())\n","\n","    val ds = spark.read.format(\"text\").load(dataPath);\n","\n","    var createdCount = 0;\n","    ds.collect().foreach(row => {\n","      var jsonString = row.getString(0)\n","      var parts = Serialization.read[CatalogPartitions](jsonString);\n","\n","      var catalogTablePartitions = new ListBuffer[CatalogTablePartition]()\n","      parts.tablePartitons.foreach( part => {\n","        catalogTablePartitions += ConvertCatalogTablePartition(part)\n","      })\n","\n","      RetriableFunc(() => {\n","        spark.sharedState.externalCatalog.createPartitions(DatabasePrefix + parts.database, TablePrefix + parts.table, catalogTablePartitions, IgnoreIfExists)\n","      })\n","\n","      createdCount+=catalogTablePartitions.size;\n","      println(createdCount +  \" partitions created\" + Calendar.getInstance().getTime())\n","    });\n","\n","    println(\"Partition Created completed. Total \" + createdCount + \" partition created. \" + Calendar.getInstance().getTime())\n","    return createdCount\n","  }\n","\n","  def LoadStats(dataPath: String): List[CatalogStat] = {\n","    println(\"Start to load stats \" + Calendar.getInstance().getTime())\n","\n","    val ds = spark.read.format(\"text\").load(dataPath);\n","\n","    var statBuffer = new ListBuffer[CatalogStat];\n","    ds.collect().foreach(row => {\n","      var jsonString = row.getString(0)\n","      statBuffer.append(Serialization.read[CatalogStat](jsonString))\n","    })\n","\n","    return statBuffer.toList\n","  }\n","\n","  def ValidateImportResult(entityType: String, createdCount: Int, stats: List[CatalogStat]):Boolean = {\n","    var mappingStat = stats.find(stat => stat.entityType == entityType && stat.database == None && stat.table == None);\n","    if (mappingStat == None) {\n","      println(\"Validated failed. Failed to get orignal \" + entityType + \" count\")\n","      return false\n","    }\n","\n","    if (mappingStat.get.count != createdCount) {\n","      println(\"Validated failed. Catalog object count missmatch. Expected \" + entityType + \" count is \" + mappingStat.get.count + \", but created \" + entityType + \" count is \" + createdCount);\n","      return false;\n","    }\n","\n","    println(\"Validated passed. Catalog objects are created as expected. \" + createdCount + \" \" + entityType + \" are created.\" )\n","    return true\n","  }\n","\n","  def ImportCatalogObjectsFromFile(inputPath: String) = {\n","\n","    val dbsPath = inputPath + \"databases\";\n","    val tablesPath = inputPath + \"tables\";\n","    val partPath = inputPath + \"partitions\";\n","\n","    CreateDatabases(dbsPath)\n","    CreateTables(tablesPath)\n","    CreatePartitions(partPath)\n","  }\n","}\n","\n","var stats = ImportMetadata.LoadStats(IntermediateFolderPath + \"/catalogObjectStats\")"]},{"cell_type":"code","execution_count":null,"id":"482efd6b-1866-42e0-b37b-7101b9d1d64e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"scala"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%%spark\n","// Validate table path\n","ImportMetadata.ValidateTablePath(IntermediateFolderPath + \"/tables\")"]},{"cell_type":"code","execution_count":null,"id":"29ca55a3-88fd-4b98-80b2-2654b3df13f3","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["// Import Databases\n","var createdDb = ImportMetadata.CreateDatabases(IntermediateFolderPath + \"/databases\")\n","ImportMetadata.ValidateImportResult(DatabaseType, createdDb, stats)"]},{"cell_type":"code","execution_count":null,"id":"d309fe7d-66ea-45dd-affe-2444c044f555","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"scala"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%%spark\n","// Validate Lakehouse (database) creation\n","spark.sharedState.externalCatalog.listDatabases()"]},{"cell_type":"code","execution_count":null,"id":"b43b002a-3926-41c6-9686-8d66b6e776cc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"scala"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%%spark\n","// Import Tables\n","var createdTbl = ImportMetadata.CreateTables(IntermediateFolderPath + \"/tables\")\n","ImportMetadata.ValidateImportResult(TableType, createdTbl, stats)"]},{"cell_type":"code","execution_count":null,"id":"f79e4008-8085-4065-8d27-898ddc2fa106","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"scala"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%%spark\n","// Import Partitions\n","var createdPart = ImportMetadata.CreatePartitions(IntermediateFolderPath + \"/partitions\")\n","ImportMetadata.ValidateImportResult(PartitionType, createdPart, stats)"]},{"cell_type":"code","execution_count":null,"id":"314a2bcc-0f32-49d9-9d0a-0edbe370ceb3","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"sparksql"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%%sql\n","DESCRIBE EXTENDED syndb1.t_manag_part_delta"]},{"cell_type":"code","execution_count":null,"id":"d4ac6f99-425d-4606-8cde-e087026ff652","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%%pyspark\n","spark.catalog.listTables()"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"scala"},"microsoft":{"host":{"synapse_widget":{"state":{},"token":"c71ec2fe-4d5f-47a7-81d5-cef34f5f506f"}},"language":"scala","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"255fdd58-6a1d-40b6-86f7-76102aab0fbd","default_lakehouse_name":"synapse","default_lakehouse_workspace_id":"d3de3f63-266f-454c-87e1-5c7c598a41dd","known_lakehouses":[{"id":"255fdd58-6a1d-40b6-86f7-76102aab0fbd"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
